{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression #Max entropy\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "negpath = \"../Data/review_polarity/txt_sentoken/neg\"\n",
    "pospath = \"../Data/review_polarity/txt_sentoken/pos\"\n",
    "#function to read all negative and positive lines into separate lists\n",
    "def read_file(path):\n",
    "    listlines = []\n",
    "    for file in os.listdir(path):\n",
    "        currfile=path+\"/\"+file\n",
    "        f = open(currfile, \"r\")\n",
    "        lines = f.read() #switched from readlines() to read the entire review\n",
    "        listlines.append(lines)\n",
    "    return listlines\n",
    "old_neglines=[]\n",
    "old_poslines=[]\n",
    "old_neglines = read_file(negpath)\n",
    "old_poslines = read_file(pospath)\n",
    "#print(old_neglines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Modification 3: Gets all words that are associated with an emotion.\n",
    "emotion_path = \"../Data/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "f = open(emotion_path, \"r\")\n",
    "emotions = f.readlines()\n",
    "emotion_words = set()\n",
    "for line in emotions:\n",
    "    emotion = line.split()\n",
    "    #If the current word has an emotion associated with it, add it to the set\n",
    "    if(emotion[2]==\"1\"):\n",
    "        emotion_words.add(emotion[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: lowercase and words only (remove punctuation, numbers)\n",
    "#lemmatized\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords minus negation\n",
    "stop_words = set(stopwords.words(\"english\"))-set([\"couldn't\",\"wouldn\",\"haven't\",\"haven\",\"aren\",\"aren't\",\"isn\",\"isn't\",\"ain\",\"wouldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"wasn't\",\"don\",\"don't\",\"couldn\",\"shouldn\",\"shouldn't\",\"hasn\",\"hasn't\",\"hadn\",\"hadn't\",\"not\",\"won\",\"won't\",])\n",
    "neg_words = \"never|nothing|nowhere|none|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint|no$|not$\"\n",
    "\n",
    "def preprocess(listlines):\n",
    "    processed = []\n",
    "    for line in listlines:\n",
    "        #reviews appear to already be in lowercase, but ensure that's the case:\n",
    "        newline = line.lower()\n",
    "        \n",
    "        ########### Modification 3: Remove sentences with no emotion words ##########\n",
    "        sentences=nltk.sent_tokenize(newline)\n",
    "        indices = [] #indices of sentences without emotion words\n",
    "        for i in range(len(sentences)):\n",
    "                templine = sentences[i].split()\n",
    "                #Doesn't deal with length-1 sentences (punctuation)\n",
    "                if len(sentences[i])>1:\n",
    "                        emo = False #tracks if any emotion words appear in sentence\n",
    "                        for word in templine:\n",
    "                                #lemmatize each word to check if lemma in emotion words\n",
    "                                lemmatizer = WordNetLemmatizer()\n",
    "                                lemword = lemmatizer.lemmatize(word)\n",
    "                                if lemword in emotion_words:\n",
    "                                        emo = True\n",
    "                                        break\n",
    "                        if emo == False:\n",
    "                                #print(sentences[i])\n",
    "                                indices.append(i)\n",
    "        #delete sentences\n",
    "        for i in reversed(indices):\n",
    "                del sentences[i]\n",
    "        #update newline to reflect changes\n",
    "        newline=\" \".join(sentences)\n",
    "        ########################################################################  \n",
    "        #remove stopwords\n",
    "        newline = newline.split()\n",
    "        newline = [word for word in newline if not word in stop_words]\n",
    "        newline = ' '.join(newline)\n",
    "\n",
    "        ########### Modification 2: Mark words following negative words ##########\n",
    "        punct = \"[.:;!?]\"\n",
    "        newline = re.sub(r\"[^a-zA-z.:;!?\\s]\",'',newline) #keep clause-level punctuation first\n",
    "        newline = newline.split()\n",
    "        #iterate through tokens and mark words. Note clause-level punctuation is its own token\n",
    "        count = 0\n",
    "        while count < len(newline):\n",
    "            #if current word is a negation, process\n",
    "            if re.search(neg_words,newline[count]) is not None:\n",
    "                count+=1\n",
    "                #mark all words until clause-level punctuation reached\n",
    "                while count < len(newline) and re.search(punct,newline[count]) is None:\n",
    "                    newline[count] = newline[count]+\"_NEG\"\n",
    "                    count+=1\n",
    "            else:\n",
    "                count+=1\n",
    "        newline = ' '.join(newline)\n",
    "        ########################################################################        \n",
    "        #remove non-alphabet characters, keeping underscore.\n",
    "        newline = re.sub(r\"[^a-zA-z\\s_]\",'',newline) \n",
    "        ########### Modification 1: Replace negation with \"not\" ##########\n",
    "        newline = re.sub(neg_words,\"not\",newline)\n",
    "        ########################################################################\n",
    "        #print(newline)\n",
    "        processed.append(newline)\n",
    "    return processed\n",
    "neglines = preprocess(old_neglines)\n",
    "poslines = preprocess(old_poslines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label each line with \"positive\" or \"negative\"\n",
    "def labellines(listlines, sentiment):\n",
    "    labeled = []\n",
    "    for line in listlines:\n",
    "        curr_line = []\n",
    "        curr_line.append(line)\n",
    "        curr_line.append(sentiment)\n",
    "        labeled.append(curr_line)\n",
    "    return labeled\n",
    "neglabeled=labellines(neglines,\"Negative\")\n",
    "poslabeled=labellines(poslines,\"Positive\")\n",
    "lines = neglabeled+poslabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                word sentiment\n",
      "0  plot  two teen couples go church party drink d...  Negative\n",
      "1  happy bastards quick movie review damn yk bug ...  Negative\n",
      "2  movies like make jaded movie viewer thankful i...  Negative\n",
      "3  quest camelot warner bros  first featurelength...  Negative\n",
      "4  synopsis  mentally unstable man undergoing psy...  Negative\n",
      "5  capsule  planet mars police taking custody acc...  Negative\n",
      "6  wholesome surveillance man loses sight values ...  Negative\n",
      "7  thats exactly long movie felt  even nine laugh...  Negative\n",
      "8  call road trip walking wounded  rd plays convi...  Negative\n",
      "9  plot  young french boy sees pnots killed_NEG e...  Negative\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(lines, columns=[\"word\",\"sentiment\"])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"sentiment\"]\n",
    "data = df.drop(columns=[\"sentiment\"])\n",
    "X = data[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From demo:\n",
    "# Define a function to take as input training and testing vectors and labels\n",
    "# Allow this to be extensible to let multiple classifiers be used here\n",
    "def buildClassifiers(clf, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # if you get a zero-devision warning message, you can supress it by setting zero_division=0\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 for Naive_Bayes:\t\t 0.8079380883785785\n",
      "Average Precision for Naive_Bayes:\t 0.8119796400847742\n",
      "Average Recall for Naive_Bayes:\t 0.810929550344745\n",
      "Average Accuracy for Naive_Bayes:\t 0.808\n",
      "Average F1 for Decision_Tree:\t\t 0.6191526614885313\n",
      "Average Precision for Decision_Tree:\t 0.6203174239600939\n",
      "Average Recall for Decision_Tree:\t 0.6201262244789335\n",
      "Average Accuracy for Decision_Tree:\t 0.62\n",
      "Average F1 for Max_Entropy:\t\t 0.8247291223489034\n",
      "Average Precision for Max_Entropy:\t 0.825198737913604\n",
      "Average Recall for Max_Entropy:\t 0.8256080354798001\n",
      "Average Accuracy for Max_Entropy:\t 0.825\n"
     ]
    }
   ],
   "source": [
    "# Construct the classifiers at hand prior to folding the data through them\n",
    "names = ['Naive_Bayes', 'Decision_Tree','Max_Entropy']#,'Support_Vector_Machines']\n",
    "classifiers = [MultinomialNB(), \n",
    "              DecisionTreeClassifier(random_state=0, criterion='gini'), LogisticRegression(random_state=0,max_iter=1000)]#, SVC(kernel='linear')]\n",
    "# names = ['Max_Entropy']\n",
    "# classifiers = [LogisticRegression(random_state=0,max_iter=1000)]\n",
    "# Try different classifiers: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    #print('Now classifying', name)\n",
    "\n",
    "    # Fold the data 5 times\n",
    "    kf = KFold(n_splits = 5, shuffle=True, random_state=0)\n",
    "    foldCounter = 0\n",
    "    aList, bList, cList, dList = list(), list(), list(), list()\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        #vectorize here:\n",
    "        #vectorizer = CountVectorizer() #comment out if using tfidf\n",
    "        ######################Modification 4: Use tfidf#####################\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        ####################################################################\n",
    "        X_train_vect = vectorizer.fit_transform(X_train).toarray()\n",
    "        X_test_vect = vectorizer.transform(X_test).toarray()\n",
    "        f1, precision, recall, accuracy = buildClassifiers(clf, X_train_vect, X_test_vect, y_train, y_test)\n",
    "        aList.append(f1)\n",
    "        bList.append(precision)\n",
    "        cList.append(recall)\n",
    "        dList.append(accuracy)\n",
    "    print(\"Average F1 for {}:\\t\\t\".format(name), np.mean(aList))\n",
    "    print(\"Average Precision for {}:\\t\".format(name), np.mean(bList))\n",
    "    print(\"Average Recall for {}:\\t\".format(name), np.mean(cList))\n",
    "    print(\"Average Accuracy for {}:\\t\".format(name), np.mean(dList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
