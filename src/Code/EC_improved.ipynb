{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression #Max entropy\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Yelp reviews to be similar to movie reviews\n",
    "import unicodedata\n",
    "reviewpath = \"../Data/ExtraCredit/Yelp/all_reviews.txt\"\n",
    "\n",
    "old_neglines=[]\n",
    "old_poslines=[]\n",
    "#function to read all negative and positive lines into separate lists\n",
    "f = open(reviewpath, mode=\"r\",encoding='utf-8')\n",
    "lines = f.read() #switched from readlines() to read the entire review\n",
    "allrev =lines.split(\"]]]\") #split each review. will need to process other brackets\n",
    "allrev.pop() #removes the last item, which is empty\n",
    "#For each review, isolate the text and put into positive or negative list\n",
    "#depending on the associated rating. 1-3 stars = negative, and 4-5 stars = positive\n",
    "for i in range(len(allrev)):\n",
    "    currlines = allrev[i].splitlines()\n",
    "    #Remove empty first line from reviews:\n",
    "    if(i>0):\n",
    "        currlines = currlines[1:]\n",
    "    positive_review = False\n",
    "    rating = int(currlines[1][0])\n",
    "    if rating > 3:\n",
    "        positive_review = True\n",
    "    #Get review text: line 7 until end, minus 3 blank lines at the end\n",
    "    currlines = currlines[7:-3]\n",
    "    #print(currlines)\n",
    "    currlines = \" \".join(currlines)\n",
    "    #Remove weird symbols like \"\\xa0\":\n",
    "    currlines = unicodedata.normalize(\"NFKD\",currlines)\n",
    "    #Add spaces between punctuation and words. Extra spaces don't matter.\n",
    "    currlines = re.sub(r\"([^\\w\\s])\",r\" \\1 \", currlines)\n",
    "\n",
    "    if positive_review:\n",
    "        old_poslines.append(currlines)\n",
    "    else:\n",
    "        old_neglines.append(currlines)\n",
    "\n",
    "#print(len(old_neglines)+len(old_poslines)) #number of reviews: 10391\n",
    "#print(len(old_neglines)) #number of negative (1-3 star) reviews:1172+1358+1795 = 4325\n",
    "#print(old_neglines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Modification 3: Gets all words that are associated with an emotion.\n",
    "emotion_path = \"../Data/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "f = open(emotion_path, \"r\")\n",
    "emotions = f.readlines()\n",
    "emotion_words = set()\n",
    "for line in emotions:\n",
    "    emotion = line.split()\n",
    "    #If the current word has an emotion associated with it, add it to the set\n",
    "    if(emotion[2]==\"1\"):\n",
    "        emotion_words.add(emotion[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: lowercase and words only (remove punctuation, numbers)\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords minus negation\n",
    "stop_words = set(stopwords.words(\"english\"))-set([\"couldn't\",\"wouldn\",\"haven't\",\"haven\",\"aren\",\"aren't\",\"isn\",\"isn't\",\"ain\",\"wouldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"wasn't\",\"don\",\"don't\",\"couldn\",\"shouldn\",\"shouldn't\",\"hasn\",\"hasn't\",\"hadn\",\"hadn't\",\"not\",\"won\",\"won't\",])\n",
    "neg_words = \"never|nothing|nowhere|none|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint|no$|not$\"\n",
    "\n",
    "def preprocess(listlines):\n",
    "    processed = []\n",
    "    for line in listlines:\n",
    "        #lowercase all reviews:\n",
    "        newline = line.lower()\n",
    "        \n",
    "        ########### Modification 3: Remove sentences with no emotion words ##########\n",
    "        # sentences=nltk.sent_tokenize(newline)\n",
    "        # indices = [] #indices of sentences without emotion words\n",
    "        # for i in range(len(sentences)):\n",
    "        #         templine = sentences[i].split()\n",
    "        #         #Doesn't deal with length-1 sentences (punctuation)\n",
    "        #         if len(sentences[i])>1:\n",
    "        #                 emo = False #tracks if any emotion words appear in sentence\n",
    "        #                 for word in templine:\n",
    "        #                         #lemmatize each word to check if lemma in emotion words\n",
    "        #                         lemmatizer = WordNetLemmatizer()\n",
    "        #                         lemword = lemmatizer.lemmatize(word)\n",
    "        #                         if lemword in emotion_words:\n",
    "        #                                 emo = True\n",
    "        #                                 break\n",
    "        #                 if emo == False:\n",
    "        #                         #print(sentences[i])\n",
    "        #                         indices.append(i)\n",
    "        # #delete sentences\n",
    "        # for i in reversed(indices):\n",
    "        #         del sentences[i]\n",
    "        # #update newline to reflect changes\n",
    "        # newline=\" \".join(sentences)\n",
    "        ########################################################################\n",
    "\n",
    "        #remove stopwords\n",
    "        newline = newline.split()\n",
    "        newline = [word for word in newline if not word in stop_words]\n",
    "        newline = ' '.join(newline)\n",
    "\n",
    "        # ########### Modification 2: Mark words following negative words ##########\n",
    "        punct = \"[.:;!?]\"\n",
    "        newline = re.sub(r\"[^a-zA-z.:;!?\\s]\",'',newline) #keep clause-level punctuation first\n",
    "        newline = newline.split()\n",
    "        #iterate through tokens and mark words. Note clause-level punctuation is its own token\n",
    "        count = 0\n",
    "        while count < len(newline):\n",
    "            #if current word is a negation, process\n",
    "            if re.search(neg_words,newline[count]) is not None:\n",
    "                count+=1\n",
    "                #mark all words until clause-level punctuation reached\n",
    "                while count < len(newline) and re.search(punct,newline[count]) is None:\n",
    "                    newline[count] = newline[count]+\"_NEG\"\n",
    "                    count+=1\n",
    "            else:\n",
    "                count+=1\n",
    "        newline = ' '.join(newline)\n",
    "        ########################################################################        \n",
    "        #remove non-alphabet characters, keeping underscore.\n",
    "        newline = re.sub(r\"[^a-zA-z\\s_]\",'',newline) \n",
    "        ########### Modification 1: Replace negation with \"not\" ##########\n",
    "        newline = re.sub(neg_words,\"not\",newline)\n",
    "        ########################################################################\n",
    "        #print(newline)\n",
    "        processed.append(newline)\n",
    "    return processed\n",
    "neglines = preprocess(old_neglines)\n",
    "poslines = preprocess(old_poslines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label each line with \"positive\" or \"negative\"\n",
    "def labellines(listlines, sentiment):\n",
    "    labeled = []\n",
    "    for line in listlines:\n",
    "        curr_line = []\n",
    "        curr_line.append(line)\n",
    "        curr_line.append(sentiment)\n",
    "        labeled.append(curr_line)\n",
    "    return labeled\n",
    "neglabeled=labellines(neglines,\"Negative\")\n",
    "poslabeled=labellines(poslines,\"Positive\")\n",
    "lines = neglabeled+poslabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                word sentiment\n",
      "0  ordered kung pao chicken came pieces chicken c...  Negative\n",
      "1  food good service awful  placed order online w...  Negative\n",
      "2  seeking decent chinese take delivery restauran...  Negative\n",
      "3  took minutes food get forgot spring roll  hung...  Negative\n",
      "4  disappointed entre e purchase  vegetables hard...  Negative\n",
      "5  not vietnamese_NEG restaurant_NEG  friend orde...  Negative\n",
      "6  place awesome jacked prices  dollars spring ro...  Negative\n",
      "7  lived champaign years want love pekara support...  Negative\n",
      "8  travelling stopped breakfast  organic eggs ome...  Negative\n",
      "9  worst crepe        european cafe don know make...  Negative\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(lines, columns=[\"word\",\"sentiment\"])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"sentiment\"]\n",
    "data = df.drop(columns=[\"sentiment\"])\n",
    "X = data[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From demo:\n",
    "# Define a function to take as input training and testing vectors and labels\n",
    "# Allow this to be extensible to let multiple classifiers be used here\n",
    "def buildClassifiers(clf, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # if you get a zero-devision warning message, you can supress it by setting zero_division=0\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 for Naive_Bayes:\t\t 0.6748063625244479\n",
      "Average Precision for Naive_Bayes:\t 0.8224607136285511\n",
      "Average Recall for Naive_Bayes:\t 0.6812179906757473\n",
      "Average Accuracy for Naive_Bayes:\t 0.7323623512266438\n",
      "Average F1 for Decision_Tree:\t\t 0.718858070191166\n",
      "Average Precision for Decision_Tree:\t 0.7180758662104312\n",
      "Average Recall for Decision_Tree:\t 0.7233627758116061\n",
      "Average Accuracy for Decision_Tree:\t 0.7228356714400987\n",
      "Average F1 for Max_Entropy:\t\t 0.822543516304471\n",
      "Average Precision for Max_Entropy:\t 0.8208692991369885\n",
      "Average Recall for Max_Entropy:\t 0.8252525471104615\n",
      "Average Accuracy for Max_Entropy:\t 0.8262909585334993\n"
     ]
    }
   ],
   "source": [
    "# Construct the classifiers at hand prior to folding the data through them\n",
    "names = ['Naive_Bayes', 'Decision_Tree','Max_Entropy']#,'Support_Vector_Machines']\n",
    "classifiers = [MultinomialNB(), \n",
    "              DecisionTreeClassifier(random_state=0, criterion='gini'), LogisticRegression(random_state=0,max_iter=1000)]#, SVC(kernel='linear')]\n",
    "# names = ['Max_Entropy']\n",
    "# classifiers = [LogisticRegression(random_state=0,max_iter=1000)]\n",
    "# Try different classifiers: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    #print('Now classifying', name)\n",
    "\n",
    "    # Fold the data 5 times\n",
    "    kf = KFold(n_splits = 5, shuffle=True, random_state=0) #changed to not need shuffle\n",
    "    foldCounter = 0\n",
    "    aList, bList, cList, dList = list(), list(), list(), list()\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        #vectorize here:\n",
    "        #vectorizer = CountVectorizer() #comment out if using tfidf\n",
    "        ######################Modification 4: Use tfidf#####################\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        ####################################################################\n",
    "        X_train_vect = vectorizer.fit_transform(X_train).toarray()\n",
    "        X_test_vect = vectorizer.transform(X_test).toarray()\n",
    "        f1, precision, recall, accuracy = buildClassifiers(clf, X_train_vect, X_test_vect, y_train, y_test)\n",
    "        aList.append(f1)\n",
    "        bList.append(precision)\n",
    "        cList.append(recall)\n",
    "        dList.append(accuracy)\n",
    "    print(\"Average F1 for {}:\\t\\t\".format(name), np.mean(aList))\n",
    "    print(\"Average Precision for {}:\\t\".format(name), np.mean(bList))\n",
    "    print(\"Average Recall for {}:\\t\".format(name), np.mean(cList))\n",
    "    print(\"Average Accuracy for {}:\\t\".format(name), np.mean(dList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
